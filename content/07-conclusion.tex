\chapter{Folgerungen}
In der vorliegenden Bachelorarbeit wurden die drei Frameworks IBM DP, Google DP und Smartnoise SDK in einem medizinischen Anwendungsfall evaluiert. Hierfür wurde eine generische Schnittstelle programmiert. Sie liest medizinische Alterswerte von mit COVID-19 infizierten Personen ein und berechnet ihren Durchschnittswert durch die entsprechende \gls{dp} Funktion der Frameworks. Anhand von Metriken werden die verrauschten Durchschnittswerte auf Privatsphäre, Genauigkeit und Erwartungstreue bewertet.

Die ausgeführte Evaluation zeigt deutlich auf, dass IBM DP in allen Kategorien stets insgesamt die besten Werte bei den Metriken erzielt hat. Hierauf folgend liegt das Framework Smartnoise SDK, welches durchschnittliche instabile Qualität in den metrischen Werten erreicht hat. Dagegen hat das Google DP Framework in allen bis auf die Metrik Standardabweichung geringe Leistung in den Metriken aufgezeigt. In der Ausführungszeit liegen IBM DP und Google DP im Sekundenbereich, erheblich entgegengesetzt liegt Smartnoise SDK im Stundenbereich. Im vorgestellten medizinischen Anwendungsfall ist IBM \gls{dp} ohne Einschränkungen einsetzbar, jedoch Smartnoise SDK aufgrund von zeitlichen Einschränkungen nicht verwendbar ist.
\section{Einsetzbarkeit}
Bei der Einsetzung eines der drei Frameworks ist zu beachten, dass Analysten in Kenntnis der Grundkonzepte von \gls{dp} sein sollten. Bei der Verwendung des Frameworks können unerwartete und widersprüchliche Resultate auftreten, die auf das Framework zurückzuführen sind. In der Evaluation dieser Bachelorarbeit wurden Mehrfachdurchführungen zur Berechnung der Metriken ausgeführt, um gewisse Muster zu erkennen und die richtigen Ergebnisse auszusortieren. Denn bei allen Frameworks kamen in manchen Metriken wie die Wasserstein-Distanz unerwartetes Verhalten auf. Somit folgen durch Analysen keine eindeutigen Ergebnisse, sondern welche mit Fehlertoleranzen.

Vor der praktischen Nutzen eines der Frameworks muss die Verbindung zum zentralen Server aufgestellt werden. Wie bereits in Kapitel 4 erwähnt, hat Smartnoise SDK als einzige die Möglichkeit eine unmittelbare Verbindung zu einer Datenbank herzustellen. Die anderen bräuchten eine zusätzliche Verbindung, um nicht auf die Rohdaten zugreifen zu müssen. Des Weiteren muss das \gls{pb} serverseitig reguliert und anpassbar sein. Der Forscher will entsprechend seiner Daten die Privatsphäre beliebig stark schützen. Bei sensiblen Attributen sowie bei Quasi-Identifikatoren, die selbst keine Identifikatoren sind, allerdings in Kombination mit Hintergrundwissen eine eindeutige Zuordnung ermöglichen, sind kleinere $\epsilon$-Werte erforderlich. Der Server muss in der Lage sein ebenfalls mit kleinen $\epsilon$ arbeiten zu können.

Eines der wichtigsten Aspekte für die Einsetzung eines der Frameworks ist die Ausführungszeit bei Anfragen. Ein Forscher stellt mehrere verschiedene Anfragen, um gewisse Erkenntnisse oder Strukturen über die Ergebnisse zu erfahren. Vor allem bei medizinischen Daten sind die Anzahl an Einträge sehr groß und vielfältig. In der Evaluation sind die Resultate zu den Ausführungszeiten der Frameworks eindeutig. IBM \gls{dp} und Google \gls{dp} können 10,000 Werte in wenigen Sekunden berechnen, wogegen Smartnoise SDK dafür ca. 2 Stunden benötigt wie in \cref{tab : classification} dargestellt. Auf dem genannten Server ist ebenfalls aufgefallen, dass die Berechnungen von Smartnoise SDK sehr viel Speicher verbraucht. Die wohl mögliche Ursache geht auf die Struktur des Frameworks zurück. Wenn eine Berechnung durchgeführt werden soll, dann ist eine SQL-Abfrage erforderlich. Zunächst wird sie in Python angegeben und durch interne Strukturen wie der Parser interpretiert und in Python ausgeführt. Dies kann zu einem großen Overhead bei der Berechnung führen und eine Erklärung für den großen Speicherverbrauch sein. Daher eignet sich selbst Smartnoise SDK auf einem Server nicht für Mehrfachanwendungen von Funktionen oder aufwendige Berechnungen.

In dieser Evaluation ist eine \gls{dp} Funktion durchgeführt worden, um weitere Erkenntnisse über die Anwendbarkeit der Frameworks im medizinischen Forschungsalltag zu erhalten, sollten weitere Funktionen folgen. Zunächst mit einfachen Funktionen wie die Summe, die Anzahl etc. und steigend Komplexere wie Laplace Mechanismen für verrauschte Einzelwerte evaluiert werden. Dies soll Aufschluss vor allem über die Ausführungszeit der Anfragen ergeben, um das Framework Smartnoise SDK doch noch einsetzen zu können.  

\section{Schutz und Nutzbarkeit der Daten}
\begin{table}[h]
	\centering
	\begin{tabular}{ l l l l} \toprule
		\textbf{Metrik} & \textbf{IBM DP} & \textbf{Google DP} & \textbf{Smartnoise SDK}  \\ \midrule
		\textit{Privatsphäre:}&&&\\ \hline
		\textit{DP-res}	& $\checkmark$ &$\checkmark$ &$\checkmark$ \\ 
		\textit{WD}	& Hoch  & Niedrig & Mittel \\
		\textit{Genauigkeit:}&&&\\ \hline
		\textit{MSE}	& Hoch & Niedrig & Hoch\\
		\textit{STD}	&  Hoch& Hoch & Mittel\\
		\textit{Erwartungstreue:}&&&\\ \hline
		\textit{MSD}	&  Hoch & Niedrig & Hoch\\ 
		\textit{Performance:}&&&\\ \hline
		\textit{Ausführungszeit}	&  Sekundenbereich & Sekundenbereich & Stundenbereich\\ \bottomrule
	\end{tabular}
	\caption{Die Einstufung der Frameworks in den Metriken aus der Evaluation.}
	\label{tab : classification}
\end{table}

In medizinischen Statistiken aus Rohdaten werden verschiedene Aspekte berücksichtigt, die anhand der Ergebnisse aus der Evaluation in \cref{tab : classification} über die Einsetzbarkeit der Frameworks Aufschluss geben soll.

Die Hauptpriorität ist stets bei Veröffentlichung von medizinischen Daten die Privatsphäre des einzelnen Patienten entsprechend der Verordnung der \gls{dsgvo}. Durch die Definition von \gls{dp} soll ein einzelner Eintrag in einer Datenmenge bis zu einem messbaren Risiko geschützt werden. Hierfür dient die Metrik DP-res, welche die Ausgabe der Frameworks mit der DP Definition validiert. Aus den Ergebnissen in \cref{tab : classification} geht hervor, dass die Ausgabe jedes Framework dies erfüllt. Ausschließlich mit dieser Erfüllung dürfen die verrauschten Daten für die medizinische Forschung genutzt werden, somit dient dies als Bedingung. Dies ist jedoch verständlich, da jedes Framework speziell für die Einhaltung von der Definition von \gls{dp} konstruiert worden ist.

In der medizinischen Forschung handelt es sich stets um sensible Daten. Manche von ihnen sind schützenswerter aufgrund ihres großen Personenbezuges. Ein gewisser Grad an Sensibilität der Daten ist vorhanden. Entsprechend diesem handelt das \gls{pb}, womit die Stärke des Schutzes der Privatsphäre reguliert werden kann. Die Qualität des Schutzes der Privatsphäre ist unterschiedlich bei den Frameworks wie in \cref{tab : classification} ersichtlich ausgefallen. Google DP hat für den kleinsten $\epsilon$-Wert sehr gut abgeschnitten, jedoch bei den restlichen sind niedrige Werte aufgetreten. Für eine medizinische Veröffentlichung kann der Schutz zu niedrig sein und zu Re-Identifizierung von Patienten führen. Smartnoise SDK leidet an Schwankungen bei kleinen $\epsilon$-Werten, was zu einer Instabilität des Schutzes führt. Vor allem für sensible Daten ist ein erwünschter hoher Schutz dadurch nicht garantiert. Insgesamt ist eine recht hoher mit relativer Instabilität an Schutz dargeboten. Schlussendlich garantiert IBM DP den Forschern ein Verhalten entsprechend ihrer Erwartungen. Ein durchgehender hoher Schutz der Privatsphäre ist vorhanden und es weist keine Instabilität bei der Ausgabe auf.

Eine ursprünglicher Gedanke ist, wenn die Privatsphäre gut geschützt ist, dann ist die Ungenauigkeit verstärkt vorhanden. Dies gilt nicht beim Smartnoise SDK und vor allem beim IBM \gls{dp}. Sie erreichen bei der Metrik MSE sehr kleine Werte, was für eine sehr geringe Abweichung vom originalen Durchschnitt der Rohdaten sorgt. Für die Statistik der medizinischen Daten werden dadurch die Aussagekraft und semantische Richtigkeit erlangt. Hingegen beim Google DP ist der Mechanismus für den schlechten Ausfall der Metrik MSE zuständig. Des Weiteren folgen bei Forschungen mehrfache Anfragen für dieselbe Thematik, dies bedeutet im Framework die mehrfache Ausführung von \gls{dp} Funktionen. Ein Forscher muss auf die Stabilität des Frameworks vertrauen können, damit die Streuung der Verrauschens nicht erheblich abhängig von der Anzahl an Anfragen ist. Diesen Aspekt deckt die Metrik STD, welche bei IBM DP und Google nahezu identisch ausfallen, ab. Smartnoise SDK stagniert bei diesem Wert, sodass die Ausgabe bei mehrfacher Anfrage minimal verändert wird. Diese Stagnation kann für kleine $\epsilon$-Werte zu geringen Schwankungen bei der Ausgabe durch das Verrauschen führen. Für die Aussagekraft von Statistiken und Forschungen ist die Genauigkeit der Daten unentbehrlich, in diesem Rahmen erfüllt IBM DP dies mit hoher Leistung, wobei Google DP große Ungenauigkeit nachweist. Dazwischen liegt das Framework Smartnoise SDK.

Nach dem theoretischen Aspekt dieser Frameworks basieren sie alle auf dem Laplace Mechanismus mit einem Erwartungswert von $0$, wodurch er im Durchschnitt mit den Rohdaten übereinstimmen soll. Bei dem Parameter $b$ für den Mechanismus bestimmt das Verhältnis zwischen der Sensitivität und dem $\epsilon$-Wert. Die Erwartungstreue der Frameworks sollte nach der Theorie vorhanden sein, da ihre Laplace Verteilung, worauf das Verrauschen basiert, zentriert ist. Damit bleibt vor allem die semantische Bedeutung der Daten durch das Verrauschen erhalten. In Forschungen werden basierend auf Daten Erkenntnisse vollzogen, die auf einem Fundament an wahrheitsgetreuen Daten gründet. Speziell in dieser Evaluation ist bei der Durchschnittsfunktion von Google \gls{dp}  ein internes Schema aufgefallen, welches zu viel von den unveränderten Daten durch das Verrauschen abzieht. Somit liegt stets der verrauschte Wert niedriger als der unveränderte Durchschnittswert. Dieses Verhalten ist durch weitere Experimente mit anderen \gls{dp} Funktionen des Frameworks zu überprüfen. Dann könnte festgestellt werden, ob stets nach diesem Schema gehandelt wird oder dies lediglich bei der Durchschnittsfunktion auftritt. Eine weitere Auffälligkeit ist, dass große $\epsilon$-Werte nicht eine höhere Erwartungstreue aufzeigen. Sondern sie werden nicht in den Berechnungen berücksichtigt. Denn, dann hat das starke Verrauschen, um die Privatsphäre zu schützen, keinen hohen Stellenwert mehr. In den anderen \gls{dp} Funktionen kann dieses Übersehen ausgeschlossen sein. Dieses Schema bleibt für weitere Forschungen offen. IBM DP und Smartnoise SDK liegen hierbei um den Wert $0$. Sie entsprechen relativ den Erwartungen vom Laplace Mechanismus und ermöglichen den Forschenden sich auf die verrauschten Ausgaben zu verlassen.

Entgegen den Erwartungen liefert das Framework IBM DP einheitlich in allen Kategorien verlässliche Ergebnisse, die für die medizinische Forschung genügen. Es ist ursprünglich für den Themenbereich maschinelles Lernen konzipiert worden, damit verschiedene Modelle auf verrauschten Daten lernen und auswerten können. Sie benötigen semantisch richtige und hohe genaue Daten innerhalb des Rahmens des Verrauschens. Dies kann ein Grund für die hohe Performance des Frameworks sein. Anschließend folgt Smartnoise SDK mit gewissen Ungereimtheiten bei kleinen $\epsilon$-Werten in den ersten beiden Kategorien. Schlussendlich schneidet Google DP in allen Metriken eher schlecht ab. Es ist für die Bewahrung der Privatsphäre von medizinischen Daten mit hoher Genauigkeit mit Vorsicht zu verwenden.

Aufgrund des Aufbaus jedes Frameworks treten gewisse unerklärbare Ergebnisse bei ihnen auf. Die interne Berechnung des Verrauschens durch den Laplace Mechanismus ist mathematisch einheitlich definiert, jedoch ist die Menge des Hinzufügen von Rauschen selbstbestimmt. Daher können gänzlich solche schlechten Ergebnisse von Google \gls{dp} in der Metrik MSD begründet werden, da diese intern vom Framework bestimmte Regulierungen folgt. Lediglich durch Abfangen von Zwischenergebnissen (siehe Kapitel 6) sind sie zurückführbar auf das Framework und stellen die Ergebnisse als valide da. Trotzdem zeichnen sich klare Muster bzw. Schemata der Frameworks ab, die durch Mehrfachausführung abgedeckt sind. Fokus der Evaluation war die Erkennung und die Beurteilung solcher Muster

Des Weiteren erfolgt die Bewertung der Frameworks teilweise und nicht total. Solch eine Bachelorarbeit, die Frameworks auf \gls{dp} evaluiert, ist bis zum aktuellen Stand in keiner vergleichbaren Arbeit bzw. keinem Paper entstanden. Aufgrund dessen fehlen Vergleichswerte als Richtlinien, um die Ergebnisse dieser Evaluation einheitlich bzw. genau zuzuordnen können. 

Es ergibt sich weiterer Forschungsbedarf, aus der Bewertung von Frameworks Richtwerte für die Metriken zu bestimmen. Dafür sollte eine großflächige Durchführung von \gls{dp} Funktionen mit verschiedenen $\epsilon$-Werten für einige Frameworks durchgeführt werden. Am Ende könnten die Resultate durch Analyse und Vergleich zu groben Richtlinien führen. Ein anderer Ansatz ist es, durch verschiedene Rohdaten Rückschlüsse auf das Verhalten der Mechanismen von den Frameworks zu ziehen. In dieser Evaluation ist die Eingabe ganzzahlige Alterswerte gewesen, sodass Gleitkommazahlen andere Ergebnisse aufgrund von technischen Gründen liefern können.